import random


que1=["입력이 음수일 때는 0을 출력하지만 양수일 때는 양수 값을 그대로 출력하는 활성화 함수","환경이라는 개념과 상호작용하며 보상 함수를 통해 에이전트의 성능을 향상시킨다","레이블된 훈련데이터를 활용하여 모델을 학습시켜 본적 없는 가까운 미래데이터에 대해 예측하는 것", "과거의 관측을 기반으로 새로운 샘플의 범주형 클래스 레이블을 예측하는 것", "데이터가 주어졌을 때, 연속적인 출력값을 예측하는 기법", "레이블 되지 않은 데이터, 구조를 알 수 없는 데이터를 다루는 것", "사정 정보 없이 쌓여 있는 그룹 정보를 의미있는 서브그룹 또는 클러스터로 조직하는 탐색적 데이터 분석 기법","고차원의 데이터를 저차원으로 축소하는 기법","오차의 합계를 알려주는 식","목적함수의 값을 최소화시키기 위해 마치 경사를 내려가듯 최솟값을 찾는 기법","고정된 크기의 훈련 데이터 셋 차원이 늘어남에 따라 특성 공간이 점점 희소해지는 현상","부모노드와 자식노드 불순도 합의 차이","최적인 클러스터 K를 추정하여 왜곡값이 빠르게 증가하는 지점의 K값을 찾는 것","클러스터 내 데이터들이 얼마나 조밀하게 모여있는지를 측정하는 그래프 도구"]
que2=["에이전트가 특정상태에서 특정 행동을 했을 때 주어지는 보상의 기댓값을 정의하는 함수","순차적으로 계속 행동을 결정해야하는 문제를 수학적으로 정의 한 것","에이전트의 의사결정을 반영하며 에이전트에게 반영된 정보를 주는 역할","의사결정을 하기 위한 관측값, 행동, 보상을 가공한 정보로, 에이전트는 이를 기반으로 의사결정을 진행","환경에서 제공해주는 정보","초기상태에서 에이전트가 미래에 받을 보상을 현재가치로 환산하여 효율적인 판단을 할 수 있도록 하는 값으로 0과1 사이의 값으로 구성된다.","에이전트에게 무작위로 움직이게 설정하여 여러 경로를 시도해 보라는 개념의 MDP","학습된 결과에 따라 에이전트의 행동을 결정하는 기법","강화학습 만의 독특한 어려움을 해결하기 위한 방법으로 행동의 가치를 추정하고 추정값으로부터 행동을 선택","추정가치가 최대인 행동 중 하나를 선택하는 것","대부분의 시간동안에는 탐욕적 선택을 수행하고 아주 가끔 상대적 빈도수를 작은 값으로 유지하면서 탐욕적 선택 대신 모든 행동을 대상으로 무작위 선택을 하는 것"]
que3=["입력으로는 늘 한쪽으로 치우처진 고정 값이며, 입력으로는 받은 값이 0인 경우에도 아무런 학습을 하지 못하는 것을 방지하는 것은?","그래프 모양은 S자 형태를 띠며 어떠한 입력값이 들어와도 0~1사이의 값을 반환","실제 결과값과 신경망이 예측한 결과값을 가지고 거리 점수를 계산하여 이 특정 사례에서 신경망이 얼마나 잘 수행되었는지 계산해 낸 것","딥러닝에서 딥이란?","머신러닝의 특정하위 분야로, 표현들을 점점 더 의미있게 만들어가는 연속 계층들을 학습하게 하는 데 중점을 두고, 데이터 표현을 학습하는 새로운 방법","더 나은 표현을 자동으로 찾아내는 과정","데이터를 대표하게 하거나 데이터를 부호화 하기 위해 데이터를 색다르게 살펴보는 방법","활용의 기본적인 방법 중 하나로 가장 큰 보상을 줄 것이라 기대하는 행동만 선택하는 것","주로 이미지 인식을 위한 딥러닝 방식","주로 음성 및 글자와 관련된 부분의 딥러닝 방식"]

que=que1+que2+que3

ans={"입력으로는 늘 한쪽으로 치우처진 고정 값이며, 입력으로는 받은 값이 0인 경우에도 아무런 학습을 하지 못하는 것을 방지하는 것은?":"편향","입력이 음수일 때는 0을 출력하지만 양수일 때는 양수 값을 그대로 출력하는 활성화 함수":"ReLU","그래프 모양은 S자 형태를 띠며 어떠한 입력값이 들어와도 0~1사이의 값을 반환":"로지스틱 시그모이드 함수","환경이라는 개념과 상호작용하며 보상 함수를 통해 에이전트의 성능을 향상시킨다":"강화학습","실제 결과값과 신경망이 예측한 결과값을 가지고 거리 점수를 계산하여 이 특정 사례에서 신경망이 얼마나 잘 수행되었는지 계산해 낸 것":"손실함수","딥러닝에서 딥이란?":"연속된 표현 층","머신러닝의 특정하위 분야로, 표현들을 점점 더 의미있게 만들어가는 연속 계층들을 학습하게 하는 데 중점을 두고, 데이터 표현을 학습하는 새로운 방법":"딥러닝","더 나은 표현을 자동으로 찾아내는 과정":"학습","데이터를 대표하게 하거나 데이터를 부호화 하기 위해 데이터를 색다르게 살펴보는 방법":"표현","주로 이미지 인식을 위한 딥러닝 방식":"CNN","주로 음성 및 글자와 관련된 부분의 딥러닝 방식":"순환신경망","활용의 기본적인 방법 중 하나로 가장 큰 보상을 줄 것이라 기대하는 행동만 선택하는 것":"탐욕적 방법","레이블된 훈련데이터를 활용하여 모델을 학습시켜 본적 없는 가까운 미래데이터에 대해 예측하는 것":"지도학습","과거의 관측을 기반으로 새로운 샘플의 범주형 클래스 레이블을 예측하는 것":"분류", "데이터가 주어졌을 때, 연속적인 출력값을 예측하는 기법":"회귀", "레이블 되지 않은 데이터, 구조를 알 수 없는 데이터를 다루는 것":"비지도학습", "사정 정보 없이 쌓여 있는 그룹 정보를 의미있는 서브그룹 또는 클러스터로 조직하는 탐색적 데이터 분석 기법":"군집","고차원의 데이터를 저차원으로 축소하는 기법":"차원축소","오차의 합계를 알려주는 식":"목적함수","목적함수의 값을 최소화시키기 위해 마치 경사를 내려가듯 최솟값을 찾는 기법":"경사하강법","고정된 크기의 훈련 데이터 셋 차원이 늘어남에 따라 특성 공간이 점점 희소해지는 현상":"차원의 저주","부모노드와 자식노드 불순도 합의 차이":"정보이득","최적인 클러스터 K를 추정하여 왜곡값이 빠르게 증가하는 지점의 K값을 찾는 것":"엘보우방법","클러스터 내 데이터들이 얼마나 조밀하게 모여있는지를 측정하는 그래프 도구":"실루엣분석","에이전트가 특정상태에서 특정 행동을 했을 때 주어지는 보상의 기댓값을 정의하는 함수":"보상함수","순차적으로 계속 행동을 결정해야하는 문제를 수학적으로 정의 한 것":"MDP","에이전트의 의사결정을 반영하며 에이전트에게 반영된 정보를 주는 역할":"환경","의사결정을 하기 위한 관측값, 행동, 보상을 가공한 정보로, 에이전트는 이를 기반으로 의사결정을 진행":"상태","환경에서 제공해주는 정보":"관측","초기상태에서 에이전트가 미래에 받을 보상을 현재가치로 환산하여 효율적인 판단을 할 수 있도록 하는 값으로 0과1 사이의 값으로 구성된다.":"감가율","에이전트에게 무작위로 움직이게 설정하여 여러 경로를 시도해 보라는 개념의 MDP":"탐험","학습된 결과에 따라 에이전트의 행동을 결정하는 기법":"활용","강화학습 만의 독특한 어려움을 해결하기 위한 방법으로 행동의 가치를 추정하고 추정값으로부터 행동을 선택":"행동가치방법","추정가치가 최대인 행동 중 하나를 선택하는 것":"가장 간단한 행동 선택 규칙","대부분의 시간동안에는 탐욕적 선택을 수행하고 아주 가끔 상대적 빈도수를 작은 값으로 유지하면서 탐욕적 선택 대신 모든 행동을 대상으로 무작위 선택을 하는 것":"입실론-탐욕적방법"}

while True:
  b=random.choice(que)
  print(b)
  a=input()
  if ans[b]==a:
    print(a)
    que.remove(b)
  else:
    print(f'틀렸습니다. 정답은 {ans[b]}')
  if a=="z" or len(que)==0:
    print("끝났습니다.")
    break